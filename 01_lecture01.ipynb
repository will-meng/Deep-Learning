{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Run this cell to define useful Latex macros)**\n",
    "\\\\[\n",
    "\\newcommand{\\card}[1]{\\left\\lvert#1\\right\\rvert}\n",
    "\\newcommand{\\condbar}[0]{\\,\\big|\\,}\n",
    "\\newcommand{\\eprob}[1]{\\widehat{\\text{Pr}}\\left[#1\\right]}\n",
    "\\newcommand{\\fpartial}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\ffpartial}[2]{\\frac{\\partial^2 #1}{\\partial {#2}^2}}\n",
    "\\newcommand{\\gradient}[0]{\\nabla}\n",
    "\\newcommand{\\norm}[1]{\\left\\lvert\\left\\lvert#1\\right\\rvert\\right\\rvert}\n",
    "\\newcommand{\\prob}[1]{\\text{Pr}\\left[#1\\right]}\n",
    "\\newcommand{\\pprob}[2]{\\text{Pr}_{#1}\\left[#2\\right]}\n",
    "\\newcommand{\\set}[1]{\\left\\{#1\\right\\}}\n",
    "\\newcommand{\\trans}[1]{#1^\\mathsf{T}}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 01: Logistic Regression\n",
    "\n",
    "In the previous lecture we saw how to learn a *regression* model. Regression problems involve predicting a continuous valued variable with range $-\\infty, \\infty$.\n",
    "\n",
    "Another kind of problem is called *classification*. *Binary* classification is the simplest kind of classification problem. Binary classification problems have YES or NO answers. For instance, \"Is this a photo of a cat?\" \"Will this patient die in the next 3 months?\"\n",
    "\n",
    "Classification problems can have more than one *class*. For instance, the [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database) is a dataset of 70,000 images of handwritten numerals: 0 through 9. Each image belongs to one of *ten* classes: it is either a zero, or a one, or a two, et cetera.\n",
    "\n",
    "We could *try* to cast classification as a regression. For instance, we could try to use a linear regression model, and try to train it so that if the image is a 9, the linear model outputs 9.0. That will work very poorly though.\n",
    "\n",
    "The first problem is this. What does it mean if your linear regression model -7? Or 33? Or 3.75? Do you round to the closest class? But then every negative number maps to class 0, whereas only the range $(4.5, 5.5)$ maps to class 5. That certainly seems unfair.\n",
    "\n",
    "Also: let's say for an example $x$ your linear model outputs 3.5, but the correct class is $y = 5$. You would like to increase the output of your linear model, so that it is closer to guessing the right answer. But by increasing the output, you are also making the model think that the answers $y = 4$ and $y = 6$ are more \"likely,\" too.\n",
    "\n",
    "The problem is this: you're putting the classes on a one-dimensional continuum, when in fact they are discrete. There is no way to slowly change a 0 digit into a 1, and then keep changing like that from a 1 to a 2. That's fundamentally why trying to cast this problem as regression is so wrong.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will have to install Keras for this to work! `pip install keras`. You may wish to check using `which pip` that you are using the Anaconda installed version of the pip command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "25000\n",
      "[1, 14, 22, 16, 43, 530, 973, 2, 2, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 2, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 2, 2, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 2, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 2, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 2, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 2, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "TOP_N_WORDS = 1000\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words = TOP_N_WORDS)\n",
    "\n",
    "print(len(x_train))\n",
    "print(len(x_test))\n",
    "\n",
    "print(x_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have here is a training set of 25,000 IMDB reviews, each marked as either 0 (negative sentiment) or 1 (positive sentiment). The reviews are encoded as a sequence of integers. Each integer stands for a word. So the sequence of words in a review are translated into a sequence of numbers.\n",
    "\n",
    "The most frequent word in the dataset is assigned the number 1: presumably this is the word \"the.\" The second most frequent word is assigned the number 2, et cetera. We have thrown out all but the thousand most common words.\n",
    "\n",
    "For this simple project, we will not use either the order of the words in a review, nor how frequently a word appears. This is quite a loss of information, but we will see that we can still do quite a good job!\n",
    "\n",
    "We will convert the variable-length sequences into fixed-length vectors of length 1,000. A review will have the value 1 at position $k$ if word $k$ is present somewhere in the review. The review will have a zero at position $k$ if the word is not present anywhere in the review.\n",
    "\n",
    "This is called a *bag of words* model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Transform dataset from variable-length word sequences to a binary valued dense matrix.\n",
    "new_x_train = np.zeros((len(x_train), TOP_N_WORDS + 1))\n",
    "# We'll use a dummy column 0 to apply an intercept theta_0 to our model. It will always have value 1.\n",
    "new_x_train[:, 0] = 1.0\n",
    "\n",
    "for example_idx, word_sequence in enumerate(x_train):\n",
    "    for word_idx in word_sequence:\n",
    "        new_x_train[example_idx, word_idx] = 1\n",
    "\n",
    "new_x_test = np.zeros((len(x_test), TOP_N_WORDS + 1))\n",
    "new_x_test[:, 0] = 1.0\n",
    "for example_idx, word_sequence in enumerate(x_test):\n",
    "    for word_idx in word_sequence:\n",
    "        new_x_test[example_idx, word_idx] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem we're going to learn a different weight $\\theta_i \\in (-\\infty, \\infty)$ for each of the $i$ top words. We'll use what's called a *generalized linear model:*\n",
    "\n",
    "\\\\[\n",
    "f(x) = g\\left( \\sum_{i = 0}^N \\theta_i x_i \\right)\n",
    "\\\\]\n",
    "\n",
    "This is a generalized linear model because we first compute a linear function of the $x_i$, and *then* we put it through some kind of other function $g$. If $g(x) = x$, we would just be back to a regular linear model. That would give us output values in the range $(-\\infty, \\infty)$, which I've said I don't want.\n",
    "\n",
    "For this binary classification problem, I would like $f(x)$ to ideally be $\\prob{Y = 1 \\condbar X = x}$. That is: I want $f(x)$ to be the true probability that a review expresses a positive sentiment, given that the bag of words is $x$.\n",
    "\n",
    "In particular, to make sense as a probability, I want $g$ to squash values in the range $(-\\infty, \\infty)$ into the range $(0, 1)$. The function we use to compress the infinite range into a range of probabilities is called the *[logistic function](https://en.wikipedia.org/wiki/Logistic_function)*. It is defined as:\n",
    "\n",
    "\\\\[\n",
    "\\sigma(z) = \\frac{e^z}{1 + e^z}\n",
    "\\\\]\n",
    "\n",
    "or equivalently as\n",
    "\n",
    "\\\\[\n",
    "\\sigma(z) = \\frac{1}{e^{-z} + 1}\n",
    "\\\\]\n",
    "\n",
    "The first version has a straightforward interpretation, while the second version plays better with how computers do floating point calculations. Theoretically they are equivalent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional**: Why do we use the logistic function to do the squashing? Aren't there other functions that would squash the continuum into the range $(0, 1)$?\n",
    "\n",
    "The logistic function is related to a concept called *log odds*. Let's just think about normal *odds*. If I give you one-to-ten odds that Secretariat will win a horse race, I am saying that I think it is ten times more likely that Secretariat will win than lose. That *doesn't* mean that I think Secretariat has a 10% chance of winning. That would *one-to-nine* odds.\n",
    "\n",
    "For simplicity I will just express one-to-ten odds as 10.0. If I thought the odds were two-to-three, I would represent that as $\\frac{3}{2} = 1.5$ odds.\n",
    "\n",
    "To convert odds $o$ to a probability, I just perform the following simple calculation:\n",
    "\n",
    "\\\\[\n",
    "p = \\frac{o}{1 + o}\n",
    "\\\\]\n",
    "\n",
    "Odds can only be non-negative. Negative odds would make no sense. But what if I represented odds *in the log scale*? That is, I say the *log odds* are 3.0 if I think the odds are $e^{3.0}$. Or I could say I think the log odds are -4 if I think the odds are $e^{-4}$.\n",
    "\n",
    "How do I convert from log odds $z$ to probabilities? Simple:\n",
    "\n",
    "\\\\[\n",
    "p = \\frac{e^z}{1 + e^z}\n",
    "\\\\]\n",
    "\n",
    "As you can see, the logistic function converts log odds to probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Not optional**: Okay. We now have our binary classification problem almost all set up. Our model is parameterized by $\\theta_i$, where\n",
    "\n",
    "\\\\[\n",
    "f(x) = \\sigma\\left( \\sum_{i = 0}^M \\theta_i x_i \\right)\n",
    "\\\\]\n",
    "\n",
    "We will interpret this as the probability that an email with bag of words $x$ represents a positive sentiment. The number is always between zero and one.\n",
    "\n",
    "The next step is: how do we learn this model from our previous data? We could use the MSE loss function to optimize our choice of $\\theta_i$. The target would be for $f(x^{(i)}) = 1$ if $y^{(i)} = 1$. That is, if the review is positive, the ideal model would be to believe that this review has a 100% chance of being positive.\n",
    "\n",
    "We will instead use something called the *mean cross-entropy loss*. This is sometimes abreviated CE or XE. Here is the definition:\n",
    "\n",
    "\\\\[\n",
    "CE(f) =\n",
    "    \\frac{1}{N}\n",
    "    \\sum_{i = 0}^N\n",
    "    y^{(i)} \\left(-\\log{f(x^{(i)})}\\right)\n",
    "    +\n",
    "    (1 - y^{(i)})\\left(-\\log{\\left(1 - f(x^{(i)})\\right)}\\right)\n",
    "\\\\]\n",
    "\n",
    "This says, if $y = 1$ the review is positive. Assign an error of $-\\log f(x^{(i)})$. If $f(x^{(i)}) = 1.0$, that means that you think there is a 100% the review is positive, which is correct. $\\log 1.0 = 0.0$. Which means you assign yourself no error, since you are completely correct. But as $f(x^{(i)})$ approaches zero, you are getting more and more wrong. $-\\log f(x^{(i)})$ will get larger and larger, so you pay more error.\n",
    "\n",
    "On the other hand, if $y = 0$ the email is not positive. Here the error you give yourself is $-\\log{(1 - f(x^{(i)}))}$. Notice that everything is symmetric and flipped. If $f(x^{(i)}) = 0.0$, you are totally correct, and you will give yourself an error of zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Not optional**: Why not use the MSE? That is why not use\n",
    "\n",
    "\\\\[\n",
    "MSE(f) = \\frac{1}{N} \\sum_{i = 0}^N \\left(y^{(i)} - f(x^{(i)})\\right)^2\n",
    "\\\\]\n",
    "\n",
    "You could do this. The MSE and the CE would agree that the very best models are the ones which exactly predict $f(x^{(i)}) = y^{(i)}$. That model would have zero error under either measure.\n",
    "\n",
    "But with machine learning we will never find a perfect model that gets everything 100% correct. We need to try to balance errors to choose the best non-perfect model.\n",
    "\n",
    "MSE and CE disagree on how to compare two non-perfect models. Model A might be better in MSE while model B might be better in CE.\n",
    "\n",
    "So which one is right? Here is one argument for using the cross-entropy error. It goes like this. We have observed a training dataset $X, y$. We want to choose the model $f$ which makes our past results the *most likely* results that we could have had.\n",
    "\n",
    "Let me give a comparison to explain. If I flip a coin 100 times and get 65 heads, why do I say that the best guess is that a heads flip has probability $0.65$? I argue this is because $p = 0.65$ *maximizes* the probability of the observed dataset:\n",
    "\n",
    "\\\\[\n",
    "p^{65}(1-p)^{35}.\n",
    "\\\\]\n",
    "\n",
    "Let's verify that:\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "\\fpartial{}{p} \\left(p^{65} (1-p)^{35}\\right)\n",
    "&=\n",
    "0\n",
    "\\\\\n",
    "65p^{64}(1-p)^{35} + -35p^{65}(1-p)^{34}\n",
    "&=\n",
    "0\n",
    "\\\\\n",
    "65(1-p) - 35p\n",
    "&=\n",
    "0\n",
    "\\\\\n",
    "65 - 100p\n",
    "&=\n",
    "0\n",
    "\\\\\n",
    "p\n",
    "&=\n",
    "\\frac{65}{100}\n",
    "\\end{align}\n",
    "\\\\]\n",
    "\n",
    "This principle of choosing the model that maximizes the probability of the observed prior data is called the *maximum likelihood principle* or MLE. The *likelihood* of a model is the *probability* that it assigns the observed prior data.\n",
    "\n",
    "Let's return to binary classification. Let's use $\\pprob{\\theta}{Y = 1 \\condbar X = x}$ instead of $f_\\theta(x)$, since we want to interpret $f$ as a probability. So we want to maximize the previously observed $y$ values (takin g the $x$ values as given):\n",
    "\n",
    "\\\\[\n",
    "\\prod_{i = 0}^N \\pprob{\\theta}{Y = y^{(i)} \\condbar X = x^{(i)}}\n",
    "\\\\]\n",
    "\n",
    "Now, for the inside of this product, I'll use a trick:\n",
    "\n",
    "\\\\[\n",
    "\\pprob{\\theta}{Y = y^{(i)} \\condbar X = x^{(i)}}\n",
    "=\n",
    "f(x^{(i)})^{y^{(i)}} (1 - f(x^{(i)}))^{(1 - y^{(i)})}\n",
    "\\\\]\n",
    "\n",
    "If $y^{(i)} = 1$, then the $(1 - f(x^{(i)}))$ part is raised to the zero power, and goes away. Vice versa for $y^{(i)} = 0$.\n",
    "\n",
    "So I want to maximize:\n",
    "\n",
    "\\\\[\n",
    "\\prod_{i = 0}^N f(x^{(i)})^{y^{(i)}} (1 - f(x^{(i)}))^{(1 - y^{(i)})}\n",
    "\\\\]\n",
    "\n",
    "Instead of maximizing a product, how about I maximize the log of the product? That's no different because the log function is *monotonic*: $\\log a < \\log b$ iff $a < b$.\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "\\log \\prod_{i = 0}^N f(x^{(i)})^{y^{(i)}} (1 - f(x^{(i)}))^{(1 - y^{(i)})}\n",
    "&=\n",
    "\\sum_{i = 0}^N \\log \\big( f(x^{(i)})^{y^{(i)}} (1 - f(x^{(i)}))^{(1 - y^{(i)})} \\big)\n",
    "\\\\\n",
    "&=\n",
    "\\sum_{i = 0}^N\n",
    "    \\log \\big( f(x^{(i)})^{y^{(i)}} \\big)\n",
    "    +\n",
    "    \\log \\big((1 - f(x^{(i)}))^{(1 - y^{(i)})} \\big)\n",
    "\\\\\n",
    "&=\n",
    "\\sum_{i = 0}^N\n",
    "    y^{(i)} \\log \\left( f(x^{(i)}) \\right)\n",
    "    +\n",
    "    (1 - y^{(i)}) \\log \\left( 1 - f(x^{(i)}) \\right)\n",
    "\\\\\n",
    "&=\n",
    "-CE(f)\n",
    "\\end{align}\n",
    "\\\\]\n",
    "\n",
    "What is this saying? It says: maximizing the maximum likelihood is the same as minimizing the cross-entropy loss.\n",
    "\n",
    "Why not just directly maximize the likelihood? The reason is that products of probabilities are really bad for floating-point math, because multiplying many numbers $< 1$ quickly yields very small likelihoods. And floating-point math on your CPU has very poor precision as the magnitude of the number becomes smaller and smaller.\n",
    "\n",
    "Working in the log space involves *adding* many (negative) logs of probabilities. This is much more accurate for floating-point math.\n",
    "\n",
    "If you like, you can explore the wiki page on [Shannon Entropy](https://en.wikipedia.org/wiki/shannon_entropy). But this is very optional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay. We now have an error function to optimize. We'll use the same strategy as last time: we'll start with a random choice of $\\theta$ and perform gradient descent.\n",
    "\n",
    "That means we need a gradient for:\n",
    "\n",
    "\\\\[\n",
    "y^{(i)} \\left(-\\log \\left( f(x^{(i)}) \\right)\\right)\n",
    "    +\n",
    "    (1 - y^{(i)}) \\left(-\\log \\left( 1 - f(x^{(i)}) \\right)\\right)\n",
    "\\\\]\n",
    "\n",
    "where\n",
    "\n",
    "\\\\[\n",
    "f(x) = \\sigma\\left( \\sum_{i = 0}^M \\theta_i x_i \\right)\n",
    "\\\\]\n",
    "\n",
    "Now, if you do some calculus you can show that $\\fpartial{}{z} \\sigma(z) = (1 - \\sigma(z))\\sigma(z)$. You may verify this if you like. It only involves chain rule and differentiation of fractions and exponentionals.\n",
    "\n",
    "Likewise, $\\fpartial{}{z} \\log \\sigma(z) = \\sigma(z)^{-1} (1 - \\sigma(z))\\sigma(z) = 1 - \\sigma(z)$. Again, via chain rule and differentiation of logs. It may be verified that $\\fpartial{}{z} \\log \\left(1 - \\sigma(z)\\right) = -\\sigma(z)$.\n",
    "\n",
    "I will use $z^{(i)} = \\sum_i \\theta_i x_i$ to simplify things a little bit. Let's attack the partial derivative of the error with respect to a parameter $\\theta_k$:\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "\\fpartial{}{\\theta_k} \\left[\n",
    "    y^{(i)} \\left(-\\log \\left( f(x^{(i)}) \\right)\\right)\n",
    "    +\n",
    "    (1 - y^{(i)}) \\left(-\\log \\left( 1 - f(x^{(i)}) \\right)\\right)\n",
    "\\right]\n",
    "&\n",
    "\\\\\n",
    "=\n",
    "\\fpartial{}{z^{(i)}} \\left[\n",
    "    y^{(i)} \\left(-\\log \\left( \\sigma(z) \\right)\\right)\n",
    "    +\n",
    "    (1 - y^{(i)}) \\left(-\\log \\left( 1 - \\sigma(z) \\right)\\right)\n",
    "\\right]\n",
    "\\fpartial{z^{(i)}}{\\theta_k}\n",
    "&\n",
    "\\\\\n",
    "=\n",
    "-\\left[\n",
    "    y^{(i)} (1 - \\sigma(z^{(i)}))\n",
    "    -\n",
    "    (1 - y^{(i)}) \\sigma(z^{(i)})\n",
    "\\right]\n",
    "\\fpartial{z^{(i)}}{\\theta_k}\n",
    "&\n",
    "\\\\\n",
    "=\n",
    "-\\left[\n",
    "    y^{(i)} - \\sigma(z^{(i)})\n",
    "\\right]\n",
    "\\fpartial{z^{(i)}}{\\theta_k}\n",
    "&\n",
    "\\end{align}\n",
    "\\\\]\n",
    "\n",
    "And of course as $z^{(i)} = \\sum_i \\theta_i x_i$, we have $\\fpartial{z^{(i)}}{\\theta_k} = x_k$. Therefore:\n",
    "\n",
    "\\\\[\n",
    "\\fpartial{}{\\theta_k} CE \n",
    "=\n",
    "\\frac{1}{N}\n",
    "\\sum_{i = 1}^N\n",
    "    -\\left[\n",
    "        y^{(i)} - \\sigma(z^{(i)})\n",
    "    \\right]\n",
    "    x_i\n",
    "=\n",
    "\\frac{1}{N}\n",
    "\\sum_{i = 1}^N\n",
    "    \\left[\n",
    "        \\sigma(z^{(i)}) - y^{(i)}\n",
    "    \\right]\n",
    "    x_i\n",
    "\\\\]\n",
    "\n",
    "Well, there's nothing left but to write up the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigma(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Notice how this can take either a vector x, or a data matrix of x rows.\n",
    "def predict_probability(X, thetas):\n",
    "    return sigma(np.dot(X, thetas))\n",
    "\n",
    "def accuracy(X, y, thetas):\n",
    "    predictions = np.round(predict_probability(X, thetas))\n",
    "    # np.abs(y - prediction) is 1 when y != prediction\n",
    "    error_rate = np.sum(np.abs(y - predictions)) / X.shape[0]\n",
    "    return 1 - error_rate\n",
    "\n",
    "def avg_cross_entropy(X, y_values, thetas):\n",
    "    num_examples = X.shape[0]\n",
    "    probabilities = predict_probability(X, thetas)\n",
    "\n",
    "    result = 0.0\n",
    "    result += np.sum(y_values * (-np.log(probabilities)))\n",
    "    result += np.sum((1 - y_values) * (-np.log(1 - probabilities)))\n",
    "\n",
    "    return result / num_examples\n",
    "\n",
    "def deriv_wrt_theta_k(X, y_values, thetas, k):\n",
    "    num_examples = X.shape[0]\n",
    "    probabilities = predict_probability(X, thetas)\n",
    "    result = np.sum((probabilities - y_values) * X[:, k])\n",
    "    return result / num_examples\n",
    "\n",
    "def gradient_wrt_thetas(X, y_values, thetas):\n",
    "    gradient = np.zeros_like(thetas)\n",
    "    for k in range(len(thetas)):\n",
    "        gradient[k] = deriv_wrt_theta_k(X, y_values, thetas, k)\n",
    "        \n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of calculating the partial derivatives summing over all the datapoints in the training set, I will break the training set into *batches* of 128 examples. For each batch I'll calculate the derivative on just those 128 examples.\n",
    "\n",
    "This means, for each pass through the data, I am updating the parameters more times. This means my model can change faster. A pass through all the data is called an *epoch*.\n",
    "\n",
    "The gradient on a batch of 128 examples will not be exactly equal to the gradient on the entire dataset. We call it a *noisy estimate*. The expected value of the gradient on the batch will be equal to the true gradient. Because we are doing gradient descent with a random estimate of the gradient, this version of gradient descent is called *Stochastic Gradient Descent* or SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_EXAMPLES = new_x_train.shape[0]\n",
    "def run_batch(X, y, thetas):\n",
    "    new_thetas = thetas - LEARNING_RATE * gradient_wrt_thetas(X, y, thetas)\n",
    "    return new_thetas\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "def run_epoch(X, y, thetas):\n",
    "    for start_idx in range(0, NUM_EXAMPLES, BATCH_SIZE):\n",
    "        batch_X = X[start_idx:(start_idx + BATCH_SIZE), :]\n",
    "        batch_y = y[start_idx:(start_idx + BATCH_SIZE)]\n",
    "        \n",
    "        thetas = run_batch(batch_X, batch_y, thetas)\n",
    "\n",
    "    # Calculate some statistics at the end of the epoch to see how we are doing.\n",
    "    loss = avg_cross_entropy(X, y, thetas)\n",
    "    acc = accuracy(X, y, thetas)\n",
    "    validation_acc = accuracy(new_x_test, y_test, thetas)\n",
    "    print(\n",
    "        f'Epoch {epoch_idx} | CE {loss:0.2f} | Acc {acc:0.2f} | Val Acc: {validation_acc:0.2f}'\n",
    "    )\n",
    "    \n",
    "    return thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial accuracy: 0.5\n",
      "Epoch 1 | CE 0.44 | Acc 0.83 | Val Acc: 0.82\n",
      "Epoch 2 | CE 0.39 | Acc 0.85 | Val Acc: 0.84\n",
      "Epoch 3 | CE 0.37 | Acc 0.85 | Val Acc: 0.85\n",
      "Epoch 4 | CE 0.35 | Acc 0.86 | Val Acc: 0.85\n",
      "Epoch 5 | CE 0.34 | Acc 0.86 | Val Acc: 0.85\n"
     ]
    }
   ],
   "source": [
    "thetas_estimate = np.zeros(new_x_train.shape[1])\n",
    "\n",
    "initial_accuracy = accuracy(new_x_train, y_train, thetas_estimate)\n",
    "print(f'Initial accuracy: {initial_accuracy}')\n",
    "LEARNING_RATE = 0.1\n",
    "NUM_EPOCHS = 5\n",
    "for epoch_idx in range(1, NUM_EPOCHS + 1):\n",
    "    thetas_estimate = run_epoch(new_x_train, y_train, thetas_estimate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 5 epochs, we have accuracy of about 85%. This is not so bad for a very simple model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (default)",
   "language": "python",
   "name": "conda-default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
